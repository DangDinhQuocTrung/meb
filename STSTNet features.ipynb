{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import utils.utils as utils\n",
    "import utils.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure everything is deterministic\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, load_data = datasets.megc(\"cropped\")\n",
    "\n",
    "uv_frames = np.load(\"../data/megc_uv_frames_secrets_of_OF.npy\")\n",
    "uv_frames = resize(uv_frames, (uv_frames.shape[0], 3, 60, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df[\"emotion\"])\n",
    "dataset = le.fit_transform(df[\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEData(Dataset):\n",
    "    def __init__(self, frames, labels, dataset, transform=None):\n",
    "        self.frames = frames\n",
    "        self.labels = labels\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.frames.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.frames[idx, ...]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        label = self.labels[idx]\n",
    "        dataset = self.dataset[idx]\n",
    "        return sample, label, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSSNet\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, output_size, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        h1 = 14\n",
    "        h2 = 28\n",
    "        h3 = 400\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=h1, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.bn1 = nn.BatchNorm2d(h1)\n",
    "        self.drop1 = nn.Dropout2d(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=h1, out_channels=h2, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(h2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.drop2 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(8 ** 2 * h2, h3)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(h3, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        features = F.relu(self.fc1(x))\n",
    "        x = self.fc2(self.drop(features))\n",
    "        x = self.softmax(x)\n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, out_channels=3, dropout=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, out_channels=3, kernel_size=3, padding=2)\n",
    "        self.conv2 = nn.Conv2d(3, out_channels=5, kernel_size=3, padding=2)\n",
    "        self.conv3 = nn.Conv2d(3, out_channels=8, kernel_size=3, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(3)\n",
    "        self.bn2 = nn.BatchNorm2d(5)\n",
    "        self.bn3 = nn.BatchNorm2d(8)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(in_features=5 * 5 * 16, out_features=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.dropout(self.maxpool(self.bn1(self.relu(self.conv1(x)))))\n",
    "        x2 = self.dropout(self.maxpool(self.bn2(self.relu(self.conv2(x)))))\n",
    "        x3 = self.dropout(self.maxpool(self.bn3(self.relu(self.conv3(x)))))\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = self.avgpool(x)\n",
    "        features = x.view(x.size(0), -1)\n",
    "        x = self.fc(features)\n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sssnet\n",
    "Total f1: 0.7617842853527486, SMIC: 0.7241661846925004, CASME2: 0.8568498168498168, SAMM: 0.6528043905093086\n",
    "RCN-A\n",
    "Total f1: 0.6821009635525764, SMIC: 0.7070837828119382, CASME2: 0.7392900050381739, SAMM: 0.5044458433748985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_frames = resize(uv_frames, (442, 3, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LOSO(uv_frames, df, epochs=200, lr=0.01, weight_decay=0.001,\n",
    "     dropout=0.5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "pred, feat = net(torch.tensor(uv_frames).to(device).float())\n",
    "feat = feat.detach().cpu()\n",
    "pred = pred.detach().cpu().max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7945578615857777"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pred, ys, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df[\"emotion\"])\n",
    "subjects = le.fit_transform(df[\"subject\"]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mat = np.concatenate([feat, labels.reshape(442, 1), subjects.reshape(442, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mat = {\"testnum\": to_mat}\n",
    "savemat(\"STSTNet_features_leak.mat\", to_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6928605588576895"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "predictions = []\n",
    "ys = []\n",
    "\n",
    "for subject in df[\"subject\"].unique():\n",
    "    train_index = df[\"subject\"] != subject\n",
    "    X_train = feat[train_index]\n",
    "    y_train = labels[train_index]\n",
    "\n",
    "    test_index = df[\"subject\"] == subject\n",
    "    X_test = feat[test_index]\n",
    "    y_test = labels[test_index]\n",
    "\n",
    "    svm = SVC(kernel=\"linear\", gamma=\"auto\")\n",
    "    svm.fit(X_train, y_train)\n",
    "    prediction = svm.predict(X_test)\n",
    "    predictions.append(prediction)\n",
    "    ys.append(y_test)\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "ys = np.concatenate(ys)\n",
    "f1 = f1_score(predictions, ys, average=\"macro\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSO(features, df, epochs=200, lr=0.01, batch_size=128, dropout=0.5, weight_decay=0.001,\n",
    "         verbose=True):\n",
    "    outputs_list = []\n",
    "    #groupby reorders elements, now the labels are in same order as outputs\n",
    "    df_groupby = pd.concat([i[1] for i in df.groupby(\"subject\")])\n",
    "    dataset_groupby = df_groupby[\"dataset\"]\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(df[\"emotion\"])\n",
    "    labels_groupby = le.transform(df_groupby[\"emotion\"])\n",
    "\n",
    "    #loop over each subject\n",
    "    for group in df.groupby(\"subject\"):\n",
    "        subject = group[0]\n",
    "        #split data to train and test based on the subject index\n",
    "        train_index = df[df[\"subject\"] != subject].index\n",
    "        X_train = features[train_index, :]\n",
    "        y_train = labels[train_index]\n",
    "        dataset_train = dataset[train_index]\n",
    "        \n",
    "        test_index = df[df[\"subject\"] == subject].index\n",
    "        X_test = features[test_index, :]\n",
    "        y_test = labels[test_index]\n",
    "        dataset_test = dataset[test_index]\n",
    "\n",
    "        #create pytorch dataloaders from the split\n",
    "        megc_dataset_train = MEData(X_train, y_train, dataset_train, None)\n",
    "        dataset_loader_train = torch.utils.data.DataLoader(megc_dataset_train,\n",
    "                                                             batch_size=batch_size, shuffle=True,\n",
    "                                                             num_workers=0)\n",
    "\n",
    "        megc_dataset_test = MEData(X_test, y_test, dataset_test, None)\n",
    "        dataset_loader_test = torch.utils.data.DataLoader(megc_dataset_test,\n",
    "                                                         batch_size=100, shuffle=False,\n",
    "                                                         num_workers=0)\n",
    "\n",
    "        \n",
    "        net = Net(df[\"emotion\"].nunique(), dropout).float().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "        net.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch in dataset_loader_train:\n",
    "                data_batch, labels_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs, _ = net(data_batch.float())\n",
    "                loss = criterion(outputs, labels_batch.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return net\n",
    "        #Test model\n",
    "        net.eval()\n",
    "        data_batch_test, labels_batch_test, _ = dataset_loader_test.__iter__().__next__()\n",
    "        data_batch_test, labels_batch_test = data_batch_test.to(device), labels_batch_test.to(device)\n",
    "        outputs, _ = net(data_batch_test.float())\n",
    "        _, prediction = outputs.max(1)\n",
    "        prediction = prediction.cpu().data.numpy()\n",
    "        outputs_list.append(prediction)\n",
    "        \n",
    "        train_outputs = net(data_batch.float())\n",
    "        _, train_prediction = train_outputs.max(1)\n",
    "        train_prediction = train_prediction.cpu().data.numpy()\n",
    "        train_f1 = f1_score(labels_batch.cpu().data.numpy(), train_prediction, average=\"macro\")\n",
    "        test_f1 = f1_score(labels_batch_test.cpu().data.numpy(), prediction, average=\"macro\")\n",
    "        \n",
    "        \n",
    "        #Print statistics\n",
    "        if verbose:\n",
    "            print(\"Subject: {}, n={} | train_f1: {:.5f} | test_f1: {:.5}\".format(\n",
    "                subject, str(labels_batch_test.shape[0]).zfill(2), train_f1, test_f1))\n",
    "            \n",
    "    outputs = np.concatenate(outputs_list)\n",
    "    f1_total = f1_score(labels_groupby, outputs, average=\"macro\")\n",
    "    idx = dataset_groupby == \"smic\"\n",
    "    f1_smic = f1_score(labels_groupby[idx], outputs[idx], average=\"macro\")\n",
    "    idx = dataset_groupby == \"casme2\"\n",
    "    f1_casme2 = f1_score(labels_groupby[idx], outputs[idx], average=\"macro\")\n",
    "    idx = dataset_groupby == \"samm\"\n",
    "    f1_samm = f1_score(labels_groupby[idx], outputs[idx], average=\"macro\")\n",
    "    print(\"Total f1: {}, SMIC: {}, CASME2: {}, SAMM: {}\".format(f1_total, f1_smic, f1_casme2, f1_samm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
