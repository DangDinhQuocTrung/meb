{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7bbf11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meb\n",
    "from meb import utils\n",
    "from meb import datasets\n",
    "from meb import core\n",
    "from meb import models\n",
    "\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit, njit\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a590c5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|███████████████████████████| 189/189 [00:21<00:00,  8.79it/s]\n",
      "Loading data: 100%|███████████████████████████| 256/256 [01:40<00:00,  2.54it/s]\n",
      "Loading data: 100%|███████████████████████████| 159/159 [01:13<00:00,  2.18it/s]\n",
      "Loading data: 100%|███████████████████████████| 267/267 [00:19<00:00, 13.85it/s]\n",
      "Loading data: 100%|███████████████████████████| 300/300 [00:52<00:00,  5.77it/s]\n",
      "Loading data: 100%|███████████████████████████| 860/860 [02:00<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:             The number of frames does not correspond for the sample             spNO.170_k\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_e\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_e2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_f\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_f2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_h2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_h3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_h4\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_h5\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.171_l\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.172_f\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.172_k\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.172_k3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.174_e\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.202_m\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.202_m2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.206_d\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.206_e2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.206_e3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.206_g3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.206_l\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.206_m\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_a\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_a2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_b\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_c4\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_c5\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_c6\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_d\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_e\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_h2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_h3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_i\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_j2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_l\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_l2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.207_l3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.208_c\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.208_c6\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.209_m\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_b2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_b3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_c2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_c3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_d\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_d2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_d3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_e\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_f\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_f2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_h\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_h2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_i2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_k\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.210_k2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.212_b\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.212_f\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.212_i\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.212_i2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.212_j\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.212_j2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.213_k\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.214_c\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.214_j\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_a3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_b5\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_d\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_d2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_d3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_d4\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_d5\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_d6\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_d7\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_e2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_f3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_f4\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_h\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_i\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_j\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_j3\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_j4\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_k2\n",
      "Warning:             The number of frames does not correspond for the sample             spNO.215_l\n",
      "Warning: The offset does not correspond for the sample            spNO.214_c\n"
     ]
    }
   ],
   "source": [
    "c = datasets.CrossDataset(resize=64, color=False, preload=True)\n",
    "df = c.data_frame\n",
    "data = c.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd9801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, video in enumerate(data):\n",
    "    data[i] = np.expand_dims(video, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2c5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate samples with less than 8 frames\n",
    "n_frames = 8\n",
    "for i, video in enumerate(data):\n",
    "    if video.shape[0] < n_frames:\n",
    "        new_shape = (n_frames,) + video.shape[1:-1]\n",
    "        video = torch.tensor(video).permute(3, 0, 1, 2).unsqueeze(0).float()\n",
    "        new_video = F.interpolate(video, size=new_shape, mode=\"trilinear\")\n",
    "        data[i] = new_video.squeeze(0).permute(1, 2, 3, 0).byte().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374ed420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LED(nn.Module):\n",
    "    \"\"\"The Learnable Eulerian Dynamics module\n",
    "    \n",
    "    The LED module constructs the LED matrix based on Eulerian video\n",
    "    magnification in a linear fashion. A tensor contraction is then\n",
    "    performed with the matrix and the input (RGB images). The output\n",
    "    is normalized to account lighting changes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 10.0,\n",
    "        r1: float = 0.4,\n",
    "        r2: float = 0.05\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.log(torch.tensor(alpha)))\n",
    "        self.r1 = nn.Parameter(torch.log(torch.tensor(r1)))\n",
    "        self.r2 = nn.Parameter(torch.log(torch.tensor(r2)))\n",
    "    \n",
    "    def _calculate_W(self, x: torch.tensor) -> torch.tensor:\n",
    "        T = x.shape[2]\n",
    "        W = x.new(T, T)\n",
    "        alpha = torch.exp(self.alpha)\n",
    "        r1 = torch.exp(self.r1)\n",
    "        r2 = torch.exp(self.r2)\n",
    "        #construct W\n",
    "        for i in range(T):\n",
    "            for j in range(T):\n",
    "                a = j - i\n",
    "                b = min(1, i)\n",
    "                if j > i:\n",
    "                    W[i, j] = alpha * (1 - r1) ** a * r1 ** b \\\n",
    "                              - alpha * (1 - r2) ** a * r2 ** b\n",
    "                elif j == i:\n",
    "                    W[i, j] = alpha * (r1 - r2)\n",
    "        return W\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        W = self._calculate_W(x)\n",
    "        out = torch.einsum(\"scfhw,fx->scxhw\", x, W)\n",
    "        div = torch.einsum(\"scfhw,fx->scxhw\", x, torch.abs(W))\n",
    "        # For stability\n",
    "        out /= div + 1\n",
    "        # Keep first frame as original\n",
    "        out[:, :, 0] = x[:, :, 0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17230823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, class_num, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.class_num = class_num\n",
    "        h1 = 32\n",
    "        h2 = 64\n",
    "        h3 = 256\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=h1, kernel_size=(1, 5, 5), stride=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3))\n",
    "        self.bn1 = nn.BatchNorm3d(h1)\n",
    "        self.drop1 = nn.Dropout3d(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(in_channels=h1, out_channels=h2, kernel_size=(2, 3, 3), stride=1)\n",
    "        self.bn2 = nn.BatchNorm3d(h2)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.drop2 = nn.Dropout3d(dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(9 ** 2 * 2 * h2, h3)\n",
    "        self.fc = nn.Linear(h3, self.class_num)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.led = LED()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.led(x)\n",
    "        x = x[:, :, 1:]\n",
    "     \n",
    "        x = self.drop1(self.bn1(self.pool(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc4ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(core.Config):\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    action_units = utils.dataset_aus[\"cross\"]\n",
    "    epochs = 400\n",
    "    batch_size = 64\n",
    "    evaluation_fn = [\n",
    "        partial(utils.MultiLabelF1Score, average=\"binary\"),\n",
    "    ]\n",
    "    train_transform = {\n",
    "        \"spatial\": None,\n",
    "        \"temporal\": datasets.NoisyUniformTemporalSubsample(6),\n",
    "    }\n",
    "    test_transform = {\n",
    "        \"spatial\": None,\n",
    "        \"temporal\": datasets.UniformTemporalSubsample(6),\n",
    "    }\n",
    "    model = partial(Net, class_num=len(action_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0edc0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEDValidator(core.CrossDatasetValidator):\n",
    "    def __init__(self, config: Config, verbose: bool = True):\n",
    "        super().__init__(config)\n",
    "        self.verbose = verbose\n",
    "    def train_model(\n",
    "        self,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        test_loader: torch.utils.data.DataLoader,\n",
    "    ) -> None:\n",
    "        \"\"\"Main training loop. Can be overriden for custom training loops.\"\"\"\n",
    "        for epoch in tqdm(range(self.cf.epochs), disable=self.disable_tqdm):\n",
    "            if epoch == 50:\n",
    "                self.optimizer.param_groups[0][\"lr\"] *= 0.1\n",
    "            self.train_one_epoch(epoch, train_loader)\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(epoch + 1)\n",
    "            if self.cf.validation_interval:\n",
    "                if (epoch + 1) % self.cf.validation_interval == 0:\n",
    "                    train_metrics = self.evaluate_model(train_loader)\n",
    "                    test_metrics, outputs_test = self.evaluate_model(\n",
    "                        test_loader, test=True\n",
    "                    )\n",
    "                    self.printer.print_train_test_validation(\n",
    "                        train_metrics, test_metrics, epoch\n",
    "                    )\n",
    "    def setup_training(self) -> None:\n",
    "        \"\"\"\n",
    "        Sets up the training modules, including model, criterion, optimizer, scheduler\n",
    "        and mixup.\n",
    "        \"\"\"\n",
    "        self.model = self.cf.model()\n",
    "        self.criterion = self.cf.criterion()\n",
    "        self.model.to(self.cf.device)\n",
    "        #self.optimizer = self.cf.optimizer(self.model.parameters())\n",
    "        #self.optimizer = self.cf.optimizer(\n",
    "        #    [\n",
    "        #    {\"params\": list(self.model.parameters())[:3], \"lr\": 0.1},\n",
    "        #    {\"params\": list(self.model.parameters())[3:]}\n",
    "        #    ]\n",
    "        #)\n",
    "        self.optimizer = self.cf.optimizer(\n",
    "            [\n",
    "            {\"params\": list(self.model.parameters())[-3:], \"lr\": 0.1},\n",
    "            {\"params\": list(self.model.parameters())[:-3]}\n",
    "            ]\n",
    "        )\n",
    "        self.scheduler = (\n",
    "            self.cf.scheduler(self.optimizer) if self.cf.scheduler else None\n",
    "        )\n",
    "        self.mixup_fn = self.cf.mixup_fn() if self.cf.mixup_fn else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d510a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████▊               | 3/5 [8:36:13<5:44:12, 10326.19s/it]/home/tvaranka/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/tvaranka/anaconda3/envs/python3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "100%|███████████████████████████████████████| 5/5 [14:20:50<00:00, 10330.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelF1Score\n",
      "AUS: ['AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'Average']\n",
      "73.2 & 69.6 & 73.7 & 51.9 & 49.6 & 54.3 & 55.2 & 53.1 & 60.6 & 64.8 & 65.9 & 64.9 & 61.4\n",
      "\n",
      "Datasets:  ['casme', 'casme2', 'samm', 'fourd', 'mmew', 'casme3a', 'Average']\n",
      "67.2 & 67.3 & 58.6 & 63.7 & 60.5 & 55.4 & 62.1\n",
      "MultiLabelF1Score\n",
      "AUS: ['AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'Average']\n",
      "52.7 & 45.7 & 63.7 & 7.9 & 0.7 & 19.3 & 13.6 & 8.5 & 26.5 & 36.7 & 33.0 & 31.7 & 28.3\n",
      "\n",
      "Datasets:  ['casme', 'casme2', 'samm', 'fourd', 'mmew', 'casme3a', 'Average']\n",
      "37.5 & 38.6 & 22.3 & 33.9 & 26.8 & 16.7 & 29.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LEDValidator(Config).validate_n_times(df, data, n_times=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
