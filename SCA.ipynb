{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "\n",
    "import utils.utils as utils\n",
    "import utils.datasets as datasets\n",
    "import utils.traditional_methods as tm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure everything is deterministic\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, load_data = datasets.cross_dataset(color=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, load_data = datasets.casme2(color=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 10\n",
    "pr_frames = []\n",
    "for i, video in enumerate(load_data):\n",
    "    idx = np.round(np.linspace(0, video.shape[-1] - 1, n_frames)).astype(\"int\")\n",
    "    video_resized = resize(video[..., idx], (112, 112, 3, 10)).transpose(2, 3, 0, 1)\n",
    "    #video_resized = video[..., idx].transpose(2, 3, 0, 1)\n",
    "    pr_frames.append(video_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix loading all data as color. Currently SAMM does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 445/1171 [14:44<43:42,  3.61s/it]  "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (462,461) into shape (462,461,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-741cde4090bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpr_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#video = resize(video, (112, 112, 3, 50))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research/tklab/personal/tvaranka/me_benchmark/utils/datasets.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research/tklab/personal/tvaranka/me_benchmark/utils/datasets.py\u001b[0m in \u001b[0;36m_get_video\u001b[0;34m(self, image_paths)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mvideo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (462,461) into shape (462,461,3)"
     ]
    }
   ],
   "source": [
    "n_frames = 50\n",
    "pr_frames = []\n",
    "for i, video in tqdm(enumerate(load_data), total=len(load_data)):\n",
    "    video = tm.tim(video.transpose(1, 2, 3, 0), n_frames)\n",
    "    #video = resize(video, (112, 112, 3, 50))\n",
    "    video = video.transpose(2, 3, 0, 1)\n",
    "    pr_frames.append(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    # 3x3x3 convolution with padding\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False)\n",
    "\n",
    "\n",
    "class Covpool(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        x = input\n",
    "\n",
    "        batchSize = x.data.shape[0]\n",
    "        dim = x.data.shape[1]\n",
    "        h = x.data.shape[2]\n",
    "        w = x.data.shape[3]\n",
    "        M = h*w\n",
    "        x = x.reshape(batchSize,dim,M)\n",
    "        I_hat = (-1./M/M)*torch.ones(M,M,device = x.device) + (1./M)*torch.eye(M,M,device = x.device)\n",
    "        I_hat = I_hat.view(1,M,M).repeat(batchSize,1,1).type(x.dtype)\n",
    "        y = x.bmm(I_hat).bmm(x.transpose(1,2))\n",
    "        ctx.save_for_backward(input,I_hat)\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input,I_hat = ctx.saved_tensors\n",
    "        x = input\n",
    "        batchSize = x.data.shape[0]\n",
    "        dim = x.data.shape[1]\n",
    "        h = x.data.shape[2]\n",
    "        w = x.data.shape[3]\n",
    "        M = h*w\n",
    "        x = x.reshape(batchSize,dim,M)\n",
    "        grad_input = grad_output + grad_output.transpose(1,2)\n",
    "        grad_input = grad_input.bmm(x).bmm(I_hat)\n",
    "        grad_input = grad_input.reshape(batchSize,dim,h,w)\n",
    "        return grad_input\n",
    "\n",
    "    \n",
    "def cov_feature(x):\n",
    "    batchsize = x.data.shape[0]\n",
    "    dim = x.data.shape[1]\n",
    "    h = x.data.shape[2]\n",
    "    w = x.data.shape[3]\n",
    "    M = h*w\n",
    "    x = x.reshape(batchsize,dim,M)\n",
    "    I_hat = (-1./M/M)*torch.ones(dim,dim,device = x.device) + (1./M)*torch.eye(dim,dim,device = x.device)\n",
    "    I_hat = I_hat.view(1,dim,dim).repeat(batchsize,1,1).type(x.dtype)\n",
    "    y = (x.transpose(1,2)).bmm(I_hat).bmm(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(\n",
    "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
    "        out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.to(device)\n",
    "\n",
    "    out = torch.autograd.Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "    \n",
    "def CovpoolLayer(var):\n",
    "    return Covpool.apply(var)\n",
    "\n",
    "\n",
    "class ResNet_multiple(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, sample_size, sample_duration,\n",
    "                 shortcut_type='A', num_classes=2, task_num=8):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet_multiple, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2),\n",
    "            padding=(3, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, layers[3], shortcut_type, stride=2)\n",
    "        \n",
    "        ############################################################\n",
    "        \n",
    "        last_duration = int(math.ceil(sample_duration / 10))\n",
    "        last_size = int(math.ceil(sample_size / 28))\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.avgpool = nn.AvgPool3d(\n",
    "            (last_duration, last_size, last_size), stride=1)\n",
    "\n",
    "        self.avgpool2d = nn.AvgPool2d(\n",
    "            (last_size, last_size), stride=1)\n",
    "\n",
    "        fc_num = 512\n",
    "        self.task_num = task_num\n",
    "\n",
    "        self.fcs = nn.ModuleList([nn.Linear(fc_num, num_classes) for _ in range(self.task_num)])\n",
    "        self.isqrt_dim = 16\n",
    " \n",
    "        self.layer_reduce = nn.Conv2d(512, self.isqrt_dim, kernel_size=1, stride=1, padding=0,\n",
    "                                          bias=False)\n",
    "        self.layer_reduce_bn = nn.BatchNorm2d(self.isqrt_dim)\n",
    "        self.layer_reduce_relu = nn.ReLU(inplace=True)\n",
    "      \n",
    "\n",
    "        last_size = 4\n",
    "        self.sp_reso = last_size * last_size\n",
    "        self.row_bn_for_spatial = nn.BatchNorm2d(self.sp_reso)\n",
    "        self.relu_normal = nn.ReLU(inplace=False)\n",
    "        self.row_conv_group_for_spatial = nn.Conv2d( \n",
    "                 self.sp_reso, self.sp_reso*4, kernel_size=(self.sp_reso, 1), \n",
    "                 groups=self.sp_reso, bias=True)\n",
    "        self.fc_adapt_channels_for_spatial = nn.Conv2d(\n",
    "                 self.sp_reso * last_size, self.sp_reso, kernel_size=1, groups=1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.ch_dim = 512  #512 2048\n",
    "        self.expansion = 1 #1\n",
    "        planes = 512  #512 2048\n",
    "        self.row_bn = nn.BatchNorm2d(self.expansion * self.ch_dim)\n",
    "        self.row_conv_group = nn.Conv2d(\n",
    "             self.ch_dim, self.ch_dim, \n",
    "             kernel_size=(self.ch_dim, 1), \n",
    "             groups = self.ch_dim, bias=True)\n",
    "        self.fc_adapt_channels = nn.Conv2d(\n",
    "             self.ch_dim, self.expansion*self.ch_dim, \n",
    "             kernel_size=1, groups=1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.conv_for_DR = nn.Conv2d(\n",
    "             planes * self.expansion, self.ch_dim, \n",
    "             kernel_size=1,stride=2, bias=True)\n",
    "        self.bn_for_DR = nn.BatchNorm2d(self.expansion*self.ch_dim)\n",
    "        self.row_bn = nn.BatchNorm2d(self.expansion*self.ch_dim)\n",
    "            #row-wise conv is realized by group conv\n",
    "\n",
    "        #####cooncat###############\n",
    "        self.groups_base = 32\n",
    "        self.groups = int(planes * self.expansion / 64)\n",
    "        self.factor = int(math.log(self.groups_base / self.groups, 2))\n",
    "        self.padding_num = self.factor + 2\n",
    "        self.conv_kernel_size = self.factor * 2 + 5\n",
    "        self.dilate_conv_for_concat1 = nn.Conv2d(planes * self.expansion, \n",
    "                                                planes * self.expansion, \n",
    "                                                kernel_size=(self.conv_kernel_size,1), \n",
    "                                                stride=1, padding=(self.padding_num,0),\n",
    "                                                groups=self.groups, bias=True)\n",
    "        \n",
    "        self.dilate_conv_for_concat2 = nn.Conv2d(planes * self.expansion, \n",
    "                                                planes * self.expansion, \n",
    "                                                kernel_size=(self.conv_kernel_size,1), \n",
    "                                                stride=1, padding=(self.padding_num,0),\n",
    "                                                groups=self.groups, bias=True)\n",
    "\n",
    "        self.bn_for_concat = nn.BatchNorm2d(planes * self.expansion)         \n",
    "        #####cooncat###############\n",
    "        #self.fc = nn.Linear(512, num_classes)     \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def chan_att(self, out):\n",
    "        out = CovpoolLayer(out) # Nxdxd\n",
    "        out = out.view(out.size(0), out.size(1), out.size(2), 1).contiguous() # Nxdxdx1\n",
    "        out = self.row_bn(out)\n",
    "        out = self.row_conv_group(out) # Nx512x1x1\n",
    "        out = self.sigmoid(out) #NxCx1x1\n",
    "        return out\n",
    "\n",
    "\n",
    "    def pos_att(self, out):\n",
    "        out = cov_feature(out) # Nx16x16\n",
    "        out = out.view(out.size(0), out.size(1), out.size(2), 1).contiguous()  # Nx16x16x1\n",
    "        out = self.row_bn_for_spatial(out)\n",
    "\n",
    "        out = self.row_conv_group_for_spatial(out) # Nx256x1x1\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc_adapt_channels_for_spatial(out) #Nx64x1x1\n",
    "        out = self.sigmoid(out) \n",
    "        out = out.view(out.size(0), 1, 4, 4).contiguous()#Nx1x8x8\n",
    "        return out\n",
    "\n",
    "\n",
    "    def downsample_long_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.Tensor(\n",
    "            out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
    "            out.size(4)).zero_()\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.to(device)\n",
    "\n",
    "        out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        residual0 = x\n",
    "        x = self.layer1(x)\n",
    "        residual1 = x\n",
    "        x = self.layer2(x)\n",
    "        residual2 = x\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x=x.squeeze(2)\n",
    "\n",
    "        pre_att = x\n",
    "        chan_att = self.chan_att(x)\n",
    "        pos_att = self.pos_att(x)\n",
    "        out1 = self.dilate_conv_for_concat1(pre_att * chan_att)\n",
    "        out2 = self.dilate_conv_for_concat2(self.relu(pre_att * pos_att))\n",
    "        out = (out1 * pre_att * chan_att) + out2 * (self.relu(pre_att * pos_att))\n",
    "        x = self.bn_for_concat(out)\n",
    "        x = self.avgpool(x)\n",
    "        feature_map = x\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        xs = [self.fcs[i](x) for i in range(self.task_num)]\n",
    "        return xs, feature_map\n",
    "    \n",
    "    \n",
    "def resnet_multiple18(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet_multiple(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleCornerCrop(object):\n",
    "    def __init__(self, scales, size, interpolation=Image.BILINEAR,\n",
    "                 crop_positions=['c', 'tl', 'tr', 'bl', 'br']):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        self.crop_positions = crop_positions\n",
    "\n",
    "    def __call__(self, img):\n",
    "        min_length = min(img.size[0], img.size[1])\n",
    "        crop_size = int(min_length * self.scale)\n",
    "\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            center_x = image_width // 2\n",
    "            center_y = image_height // 2\n",
    "            box_half = crop_size // 2\n",
    "            x1 = center_x - box_half\n",
    "            y1 = center_y - box_half\n",
    "            x2 = center_x + box_half\n",
    "            y2 = center_y + box_half\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = crop_size\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = crop_size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.crop_position = self.crop_positions[random.randint(\n",
    "            0,\n",
    "            len(self.crop_positions) - 1)]\n",
    "        \n",
    "        \n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.p < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.p = random.random()\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    \"\"\"Rescale the input PIL.Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size,\n",
    "                          int) or (isinstance(size, collections.Iterable) and\n",
    "                                   len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL.Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, int):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEGC(Dataset):\n",
    "    def __init__(self, frames, labels, transform=None):\n",
    "        self.frames = frames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.frames[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform[\"temporal\"](sample).astype(\"uint8\")\n",
    "            # Randomize parameters and make sure they are the same for all frames of a video\n",
    "            [self.transform[\"spatial\"].transforms[i].randomize_parameters() for i in range(2)]\n",
    "            # Reshape for PIL (F, H, W, C)\n",
    "            sample = sample.transpose(1, 2, 3, 0)\n",
    "            # Into PIL image for the transforms\n",
    "            sample = [self.transform[\"spatial\"](Image.fromarray(img)) for img in sample]\n",
    "            # List to torch tensor\n",
    "            sample = torch.stack(sample).permute(1, 0, 2, 3)            \n",
    "            \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, task_num):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        losses = [criterion(preds[i], labels[:, i]) for i in range(self.task_num)]\n",
    "        \n",
    "        return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskF1(nn.Module):\n",
    "    def __init__(self, task_num):\n",
    "        super(MultiTaskF1, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        \n",
    "    def calc_f1(self, label, prediction):\n",
    "        _, predicted = torch.max(prediction, 1)\n",
    "        f1 = f1_score(label.cpu(), predicted.cpu().data.numpy(), average=\"macro\")\n",
    "        return f1\n",
    "                        \n",
    "    def forward(self, preds, labels):\n",
    "        f1s = [self.calc_f1(labels[:, i], preds[i]) for i in range(self.task_num)]\n",
    "        return f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_train_transform = transforms.Compose([\n",
    "    MultiScaleCornerCrop([1.0], 112),\n",
    "    RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "spatial_test_transform = transforms.Compose([\n",
    "    Scale(112),\n",
    "    CenterCrop(112),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_train_transform(video):\n",
    "    idx = list(range(np.random.randint(0, 5), 50, 5))\n",
    "    video = video[:, idx]\n",
    "    return video\n",
    "\n",
    "def temporal_test_transform(video):\n",
    "    idx = list(range(0, 50, 5))\n",
    "    video = video[:, idx]\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tuning_parameters(model, ft_begin_index):\n",
    "    if ft_begin_index == 0:\n",
    "        return model.parameters()\n",
    "\n",
    "    ft_module_names = []\n",
    "    for i in range(ft_begin_index, 5):\n",
    "        ft_module_names.append('layer{}'.format(i))\n",
    "\n",
    "    ft_module_names.append('fc')\n",
    "    parameters = []\n",
    "    for k, v in model.named_parameters():\n",
    "        for ft_module in ft_module_names:\n",
    "            if ft_module in k:\n",
    "                parameters.append({'params': v})\n",
    "                break\n",
    "        else:\n",
    "            parameters.append({'params': v, 'lr': 0.0001})\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCA no transforms\n",
    "All AUs:  [('AU1', 0.5798611111111112), ('AU2', 0.5935505542492442), ('AU4', 0.7383783783783784), ('AU7', 0.4474885844748858), ('AU12', 0.7061680427391938), ('AU14', 0.4669603524229075), ('AU15', 0.5768795639206007), ('AU17', 0.5716308708992212)]\n",
    "Mean f1:  0.5851146822744429\n",
    "Binary f1:  0.26168866708794114\n",
    "\n",
    "temporal transform\n",
    "All AUs:  [('AU1', 0.5956208585422069), ('AU2', 0.5177536231884058), ('AU4', 0.7102490421455939), ('AU7', 0.45248868778280543), ('AU12', 0.6385220125786163), ('AU14', 0.47692307692307695), ('AU15', 0.5122606650990931), ('AU17', 0.5226216990136813)]\n",
    "Mean f1:  0.5533049581591849\n",
    "Binary f1:  0.21157106782106785\n",
    "\n",
    "temporal + spatial transform\n",
    "All AUs:  [('AU1', 0.46578366445916114), ('AU2', 0.47619047619047616), ('AU4', 0.6071092882166965), ('AU7', 0.44495412844036697), ('AU12', 0.625), ('AU14', 0.4635376853692096), ('AU15', 0.45617977528089887), ('AU17', 0.5956075435664837)]\n",
    "Mean f1:  0.5167953201904116\n",
    "Binary f1:  0.16387887471574786\n",
    "\n",
    "spatial transform\n",
    "binary 0.18...\n",
    "\n",
    "pretrained\n",
    "\n",
    "temporal + spatial transform\n",
    "All AUs:  [('AU1', 0.5368834080717488), ('AU2', 0.5535635096610707), ('AU4', 0.6813793103448276), ('AU7', 0.5105543022881083), ('AU12', 0.6359217979170473), ('AU14', 0.5896656534954408), ('AU15', 0.7489905155413654), ('AU17', 0.5678884873515746)]\n",
    "Mean f1:  0.603105873083898\n",
    "Binary f1:  0.31826910491640625\n",
    "\n",
    "no transform\n",
    "All AUs:  [('AU1', 0.49), ('AU2', 0.5026315789473685), ('AU4', 0.6451814768460575), ('AU7', 0.428235294117647), ('AU12', 0.657608695652174), ('AU14', 0.4713986464711569), ('AU15', 0.5993926247288504), ('AU17', 0.6236448115642746)]\n",
    "Mean f1:  0.5522616410409411\n",
    "Binary f1:  0.2322092966746305\n",
    "\n",
    "60epochs\n",
    "binary f1: 0.21...\n",
    "200 epochs\n",
    "All AUs:  [('AU1', 0.5381578947368422), ('AU2', 0.5316629955947136), ('AU4', 0.6443498978897209), ('AU7', 0.5208333333333334), ('AU12', 0.646462653811062), ('AU14', 0.48115386291341666), ('AU15', 0.5465599229519051), ('AU17', 0.641214933558321)]\n",
    "Mean f1:  0.5687994368486644\n",
    "Binary f1:  0.25947735784787607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 01, n=08 | train_f1: 1.0 | test_f1: 0.67298\n",
      "Subject: 02, n=13 | train_f1: 1.0 | test_f1: 0.53793\n",
      "Subject: 03, n=07 | train_f1: 1.0 | test_f1: 0.57142\n",
      "Subject: 04, n=05 | train_f1: 0.97667 | test_f1: 0.76314\n",
      "Subject: 05, n=15 | train_f1: 0.92857 | test_f1: 0.63335\n",
      "Subject: 06, n=04 | train_f1: 1.0 | test_f1: 0.60238\n",
      "Subject: 07, n=08 | train_f1: 1.0 | test_f1: 0.7619\n",
      "Subject: 08, n=03 | train_f1: 1.0 | test_f1: 0.775\n",
      "Subject: 09, n=13 | train_f1: 0.95079 | test_f1: 0.5973\n",
      "Subject: 10, n=13 | train_f1: 0.84221 | test_f1: 0.92262\n",
      "Subject: 11, n=10 | train_f1: 0.93382 | test_f1: 0.86477\n",
      "Subject: 12, n=12 | train_f1: 0.82163 | test_f1: 0.65354\n",
      "Subject: 13, n=08 | train_f1: 1.0 | test_f1: 0.5978\n",
      "Subject: 14, n=04 | train_f1: 1.0 | test_f1: 0.86667\n",
      "Subject: 15, n=03 | train_f1: 1.0 | test_f1: 0.73333\n",
      "Subject: 16, n=04 | train_f1: 1.0 | test_f1: 0.7119\n",
      "Subject: 17, n=36 | train_f1: 1.0 | test_f1: 0.47389\n",
      "Subject: 18, n=03 | train_f1: 1.0 | test_f1: 0.675\n",
      "Subject: 19, n=15 | train_f1: 1.0 | test_f1: 0.55593\n",
      "Subject: 20, n=11 | train_f1: 1.0 | test_f1: 0.84722\n",
      "Subject: 21, n=02 | train_f1: 1.0 | test_f1: 0.58333\n",
      "Subject: 22, n=02 | train_f1: 1.0 | test_f1: 1.0\n",
      "Subject: 23, n=12 | train_f1: 1.0 | test_f1: 0.65177\n",
      "Subject: 24, n=11 | train_f1: 1.0 | test_f1: 0.69581\n",
      "Subject: 25, n=05 | train_f1: 0.96643 | test_f1: 0.75446\n",
      "Subject: 26, n=16 | train_f1: 0.85 | test_f1: 0.7024\n",
      "All AUs:  [('AU1', 0.5381578947368422), ('AU2', 0.5316629955947136), ('AU4', 0.6443498978897209), ('AU7', 0.5208333333333334), ('AU12', 0.646462653811062), ('AU14', 0.48115386291341666), ('AU15', 0.5465599229519051), ('AU17', 0.641214933558321)]\n",
      "Mean f1:  0.5687994368486644\n",
      "Binary f1:  0.25947735784787607\n"
     ]
    }
   ],
   "source": [
    "action_units = [\"AU1\", \"AU2\", \"AU4\", \"AU7\", \"AU12\", \"AU14\", \"AU15\", \"AU17\"]\n",
    "idx = df[action_units].sum(1) > 0\n",
    "pr_frames2 = [pr_frames[i] for i in df[idx].index.tolist()]\n",
    "predictions = LOSO(pr_frames2, df[idx].reset_index(), action_units, epochs=200, lr=0.001, weight_decay=0.001,\n",
    "     dropout=0.5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils.utils failed: Traceback (most recent call last):\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/tvaranka/.local/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "RecursionError: maximum recursion depth exceeded\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!kill 23964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSO(features, df, action_units, epochs=200, lr=0.01, batch_size=128, dropout=0.05, weight_decay=0.001):\n",
    "    random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    torch.cuda.manual_seed(1)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    labels = np.concatenate([np.expand_dims(df[au], 1) for au in action_units], axis=1)\n",
    "    outputs_list = []\n",
    "    labels_list = []\n",
    "    for group in df.groupby(\"subject\"):\n",
    "        subject = group[0]\n",
    "        #Split data\n",
    "        train_index = df[df[\"subject\"] != subject].index\n",
    "        X_train = [features[i] for i in train_index]\n",
    "        y_train = labels[train_index]\n",
    "        \n",
    "        test_index = df[df[\"subject\"] == subject].index\n",
    "        X_test = [features[i] for i in test_index]\n",
    "        y_test = labels[test_index]\n",
    "        \n",
    "        megc_dataset_train = MEGC(X_train, y_train, {\"temporal\": temporal_train_transform, \"spatial\": spatial_train_transform})\n",
    "        #megc_dataset_train = MEGC(X_train, y_train, None)\n",
    "        dataset_loader_train = torch.utils.data.DataLoader(megc_dataset_train,\n",
    "                                                             batch_size=batch_size, shuffle=True,\n",
    "                                                             num_workers=0)\n",
    "\n",
    "        megc_dataset_test = MEGC(X_test, y_test, {\"temporal\": temporal_test_transform, \"spatial\": spatial_test_transform})\n",
    "        #megc_dataset_test = MEGC(X_test, y_test, None)\n",
    "        dataset_loader_test = torch.utils.data.DataLoader(megc_dataset_test,\n",
    "                                                         batch_size=100, shuffle=False,\n",
    "                                                         num_workers=0)\n",
    "        \n",
    "        net = resnet_multiple18(task_num=8, num_classes=2, sample_size=112, sample_duration=10).to(device)\n",
    "        pretrained_model = torch.load(\"data/resnet-18-kinetics-ucf101_split1.pth\", map_location=device)[\"state_dict\"]\n",
    "        net_state_dict = net.state_dict()\n",
    "        new_state_dict = {k[7:]: v for k, v in pretrained_model.items() if k[7:] in net_state_dict.keys()}\n",
    "        net_state_dict.update(new_state_dict)\n",
    "        net.load_state_dict(net_state_dict)\n",
    "        params_for_optimizer = get_fine_tuning_parameters(net, 4)\n",
    "        \n",
    "        criterion = MultiTaskLoss(labels.shape[1])\n",
    "        evaluation = MultiTaskF1(labels.shape[1])\n",
    "        optimizer = optim.SGD(params_for_optimizer, lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "\n",
    "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * (0.1 ** (epoch // 30))\n",
    "            for batch in dataset_loader_train:\n",
    "                data_batch, labels_batch = batch[0].to(device), batch[1].to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs, _ = net(data_batch.float())\n",
    "                loss = criterion(outputs, labels_batch.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        #eval\n",
    "        net.eval()\n",
    "        data_batch_test, labels_batch_test = dataset_loader_test.__iter__().__next__()\n",
    "        data_batch_test, labels_batch_test = data_batch_test.to(device), labels_batch_test.to(device)\n",
    "        outputs, _ = net(data_batch_test.float())\n",
    "        outputs = [output.detach().cpu() for output in outputs]\n",
    "        outputs_list.append(outputs)\n",
    "        labels_list.append(labels_batch_test.cpu())\n",
    "        train_outputs, _ = net(data_batch.float())\n",
    "        net.train()\n",
    "        f1_train = evaluation(train_outputs, labels_batch.long())\n",
    "        f1 = evaluation(outputs, labels_batch_test.long())\n",
    "        print(\"Subject: {}, n={} | train_f1: {:.5} | test_f1: {:.5}\".format(\n",
    "            subject, str(data_batch_test.__len__()).zfill(2), np.mean(f1_train), np.mean(f1)))\n",
    "    #Calculate total f1-scores\n",
    "    predictions = torch.cat([torch.tensor([torch.max(i, 1)[1].tolist() for i in outputs_list[i]]).T\n",
    "                   for i in range(outputs_list.__len__())])\n",
    "    labels = torch.cat(labels_list)\n",
    "    f1_aus = [f1_score(predictions[:, i].cpu(), labels[:, i].cpu().data.numpy(), average=\"macro\")\n",
    "              for i in range(labels.shape[1])]\n",
    "    f1_aus_binary = [f1_score(predictions[:, i].cpu(), labels[:, i].cpu().data.numpy(), average=\"binary\")\n",
    "                     for i in range(labels.shape[1])]\n",
    "    print(\"All AUs: \",list(zip(action_units, f1_aus)))\n",
    "    print(\"Mean f1: \", np.mean(f1_aus))\n",
    "    print(\"Binary f1: \", np.mean(f1_aus_binary))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
