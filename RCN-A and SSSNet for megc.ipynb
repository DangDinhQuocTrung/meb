{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import utils.utils as utils\n",
    "import utils.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure everything is deterministic\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, load_data = datasets.megc(\"cropped\")\n",
    "\n",
    "uv_frames = np.load(\"../data/megc_uv_frames_secrets_of_OF.npy\")\n",
    "uv_frames = resize(uv_frames, (uv_frames.shape[0], 3, 60, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.datasets as datasets\n",
    "df, uv_frames = datasets.megc(resize=64, optical_flow=True)\n",
    "#uv_frames = resize(uv_frames, (uv_frames.shape[0], 3, 60, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df[\"emotion\"])\n",
    "dataset = le.fit_transform(df[\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEData(Dataset):\n",
    "    def __init__(self, frames, labels, dataset, transform=None):\n",
    "        self.frames = frames\n",
    "        self.labels = labels\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.frames.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.frames[idx, ...]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        label = self.labels[idx]\n",
    "        dataset = self.dataset[idx]\n",
    "        return sample, label, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSSNet\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, output_size, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        h1 = 32\n",
    "        h2 = 64\n",
    "        h3 = 256\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=h1, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.bn1 = nn.BatchNorm2d(h1)\n",
    "        self.drop1 = nn.Dropout2d(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=h1, out_channels=h2, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(h2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.drop2 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(9 ** 2 * h2, h3)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(h3, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(self.drop(x))\n",
    "        #x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RCN-A\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"convolutional layer blocks for sequtial convolution operations\"\"\"\n",
    "    def __init__(self, in_features, out_features, num_conv, pool=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        features = [in_features] + [out_features for i in range(num_conv)]\n",
    "        layers = []\n",
    "        for i in range(len(features)-1):\n",
    "            layers.append(nn.Conv2d(in_channels=features[i], out_channels=features[i+1], kernel_size=3, padding=1, bias=True))\n",
    "            layers.append(nn.BatchNorm2d(num_features=features[i+1], affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU())\n",
    "            if pool:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        self.op = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class RclBlock(nn.Module):\n",
    "    \"\"\"recurrent convolutional blocks\"\"\"\n",
    "    def __init__(self, inplanes, planes):\n",
    "        super(RclBlock, self).__init__()\n",
    "        self.ffconv = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.rrconv = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ffconv(x)\n",
    "        y = self.rrconv(x + y)\n",
    "        y = self.rrconv(x + y)\n",
    "        out = self.downsample (y)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SpatialAttentionBlock_P(nn.Module):\n",
    "    \"\"\"linear attention block for any layers\"\"\"\n",
    "    def __init__(self, normalize_attn=True):\n",
    "        super(SpatialAttentionBlock_P, self).__init__()\n",
    "        self.normalize_attn = normalize_attn\n",
    "\n",
    "    def forward(self, l, w, classes):\n",
    "        output_cam = []\n",
    "        for idx in range(0,classes):\n",
    "            weights = w[idx,:].reshape((l.shape[1], l.shape[2], l.shape[3]))\n",
    "            cam = weights * l\n",
    "            cam = cam.mean(dim=1,keepdim=True)\n",
    "            cam = cam - torch.min(torch.min(cam,3,True)[0],2,True)[0]\n",
    "            cam = cam / torch.max(torch.max(cam,3,True)[0],2,True)[0]\n",
    "            output_cam.append(cam)\n",
    "        output = torch.cat(output_cam, dim=1)\n",
    "        output = output.mean(dim=1,keepdim=True)\n",
    "        return output\n",
    "\n",
    "def MakeLayer(block, planes, blocks):\n",
    "    layers = []\n",
    "    for _ in range(0, blocks):\n",
    "        layers.append(block(planes, planes))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"menet networks with adding attention unit\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3, dropout=0.5, num_input=3, featuremaps=64, num_layers=1, pool_size=7, version=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.version = version\n",
    "        self.classes = num_classes\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(num_input, featuremaps, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(featuremaps),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.rcls = MakeLayer(RclBlock, featuremaps, num_layers)\n",
    "        self.attenmap = SpatialAttentionBlock_P(normalize_attn=True)\n",
    "        self.downsampling = nn.AdaptiveAvgPool2d((pool_size, pool_size))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((pool_size, pool_size))\n",
    "        self.classifier = nn.Linear(pool_size*pool_size*featuremaps, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.version == 1:\n",
    "            x = self.conv1(x)\n",
    "            x = self.attenmap(x)\n",
    "            x = self.rcls(x)\n",
    "            x = self.avgpool(x)\n",
    "        if self.version == 2:\n",
    "            x = self.conv1(x)\n",
    "            x = self.attenmap(x)\n",
    "            x = self.rcls(x)\n",
    "            x = self.avgpool(x)\n",
    "        elif self.version == 3:\n",
    "            x = self.conv1(x)\n",
    "            y = self.attenmap(self.downsampling(x), self.classifier.weight, self.classes)\n",
    "            x = self.rcls(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = x * y\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet18\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, output_size, dropout=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "        self.resnet18.fc = nn.Linear(512, output_size)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.resnet18(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sssnet\n",
    "Total f1: 0.7617842853527486, SMIC: 0.7241661846925004, CASME2: 0.8568498168498168, SAMM: 0.6528043905093086\n",
    "RCN-A\n",
    "Total f1: 0.6821009635525764, SMIC: 0.7070837828119382, CASME2: 0.7392900050381739, SAMM: 0.5044458433748985\n",
    "Resnet18\n",
    "Total f1: 0.6978647449911385, SMIC: 0.6930187788311358, CASME2: 0.7663289429246877, SAMM: 0.5583756768708334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 006, n=11 | train_f1: 1.00000 | test_f1: 0.51852\n",
      "Subject: 007, n=08 | train_f1: 1.00000 | test_f1: 0.27778\n",
      "Subject: 009, n=04 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 01, n=03 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 010, n=04 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 011, n=20 | train_f1: 1.00000 | test_f1: 0.50476\n",
      "Subject: 012, n=03 | train_f1: 1.00000 | test_f1: 0.22222\n",
      "Subject: 013, n=06 | train_f1: 1.00000 | test_f1: 0.45455\n",
      "Subject: 014, n=10 | train_f1: 1.00000 | test_f1: 0.61039\n",
      "Subject: 015, n=03 | train_f1: 1.00000 | test_f1: 0.55556\n",
      "Subject: 016, n=05 | train_f1: 1.00000 | test_f1: 0.82222\n",
      "Subject: 017, n=04 | train_f1: 1.00000 | test_f1: 0.26667\n",
      "Subject: 018, n=03 | train_f1: 1.00000 | test_f1: 0.4\n",
      "Subject: 019, n=01 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 02, n=09 | train_f1: 1.00000 | test_f1: 0.53571\n",
      "Subject: 020, n=04 | train_f1: 1.00000 | test_f1: 0.44444\n",
      "Subject: 021, n=02 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 022, n=05 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 023, n=01 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 024, n=01 | train_f1: 1.00000 | test_f1: 0.0\n",
      "Subject: 026, n=09 | train_f1: 1.00000 | test_f1: 0.4\n",
      "Subject: 028, n=03 | train_f1: 1.00000 | test_f1: 0.33333\n",
      "Subject: 03, n=05 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 030, n=03 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 031, n=01 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 032, n=04 | train_f1: 1.00000 | test_f1: 0.42857\n",
      "Subject: 033, n=05 | train_f1: 1.00000 | test_f1: 0.33333\n",
      "Subject: 034, n=03 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 035, n=08 | train_f1: 1.00000 | test_f1: 0.66667\n",
      "Subject: 036, n=01 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 037, n=01 | train_f1: 1.00000 | test_f1: 0.0\n",
      "Subject: 04, n=02 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 05, n=06 | train_f1: 1.00000 | test_f1: 0.84127\n",
      "Subject: 06, n=04 | train_f1: 1.00000 | test_f1: 0.55556\n",
      "Subject: 07, n=05 | train_f1: 1.00000 | test_f1: 0.25\n",
      "Subject: 08, n=01 | train_f1: 1.00000 | test_f1: 0.0\n",
      "Subject: 09, n=10 | train_f1: 1.00000 | test_f1: 0.62963\n",
      "Subject: 11, n=04 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 12, n=11 | train_f1: 1.00000 | test_f1: 0.73016\n",
      "Subject: 13, n=02 | train_f1: 1.00000 | test_f1: 0.33333\n",
      "Subject: 14, n=03 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 15, n=03 | train_f1: 1.00000 | test_f1: 0.55556\n",
      "Subject: 16, n=03 | train_f1: 1.00000 | test_f1: 0.66667\n",
      "Subject: 17, n=31 | train_f1: 1.00000 | test_f1: 0.83633\n",
      "Subject: 19, n=11 | train_f1: 1.00000 | test_f1: 0.64286\n",
      "Subject: 20, n=02 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 21, n=01 | train_f1: 1.00000 | test_f1: 0.0\n",
      "Subject: 22, n=02 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: 23, n=08 | train_f1: 1.00000 | test_f1: 0.46667\n",
      "Subject: 24, n=03 | train_f1: 1.00000 | test_f1: 0.55556\n",
      "Subject: 25, n=05 | train_f1: 1.00000 | test_f1: 0.28571\n",
      "Subject: 26, n=11 | train_f1: 1.00000 | test_f1: 0.31579\n",
      "Subject: s1, n=06 | train_f1: 1.00000 | test_f1: 0.38889\n",
      "Subject: s11, n=07 | train_f1: 1.00000 | test_f1: 0.82222\n",
      "Subject: s12, n=09 | train_f1: 1.00000 | test_f1: 0.61905\n",
      "Subject: s13, n=10 | train_f1: 1.00000 | test_f1: 0.27451\n",
      "Subject: s14, n=10 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: s15, n=04 | train_f1: 1.00000 | test_f1: 0.55556\n",
      "Subject: s18, n=07 | train_f1: 1.00000 | test_f1: 0.51667\n",
      "Subject: s19, n=02 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: s2, n=06 | train_f1: 1.00000 | test_f1: 1.0\n",
      "Subject: s20, n=22 | train_f1: 1.00000 | test_f1: 0.64762\n",
      "Subject: s3, n=39 | train_f1: 1.00000 | test_f1: 0.55883\n",
      "Subject: s4, n=19 | train_f1: 1.00000 | test_f1: 0.47619\n",
      "Subject: s5, n=02 | train_f1: 1.00000 | test_f1: 0.33333\n",
      "Subject: s6, n=04 | train_f1: 1.00000 | test_f1: 0.38889\n",
      "Subject: s8, n=13 | train_f1: 1.00000 | test_f1: 0.7451\n",
      "Subject: s9, n=04 | train_f1: 1.00000 | test_f1: 0.73333\n",
      "Total f1: 0.6980822729288988, SMIC: 0.6846654192283702, CASME2: 0.7329382223699272, SAMM: 0.6186463349384698\n"
     ]
    }
   ],
   "source": [
    "LOSO(uv_frames, df, epochs=200, lr=0.01, weight_decay=0.001,\n",
    "     dropout=0.5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 006, n=11 | train_f1: 0.94551 | test_f1: 0.51852\n",
      "Subject: 007, n=08 | train_f1: 0.97897 | test_f1: 0.52222\n",
      "Subject: 009, n=04 | train_f1: 0.94963 | test_f1: 1.0\n",
      "Subject: 01, n=03 | train_f1: 0.81737 | test_f1: 1.0\n",
      "Subject: 010, n=04 | train_f1: 0.92511 | test_f1: 1.0\n",
      "Subject: 011, n=20 | train_f1: 0.93299 | test_f1: 0.7619\n",
      "Subject: 012, n=03 | train_f1: 0.93590 | test_f1: 0.25\n",
      "Subject: 013, n=06 | train_f1: 0.98137 | test_f1: 0.45455\n",
      "Subject: 014, n=10 | train_f1: 0.87460 | test_f1: 0.66667\n",
      "Subject: 015, n=03 | train_f1: 0.80643 | test_f1: 0.22222\n",
      "Subject: 016, n=05 | train_f1: 0.80492 | test_f1: 0.77778\n",
      "Subject: 017, n=04 | train_f1: 0.90686 | test_f1: 0.26667\n",
      "Subject: 018, n=03 | train_f1: 0.94222 | test_f1: 0.4\n",
      "Subject: 019, n=01 | train_f1: 0.85317 | test_f1: 1.0\n",
      "Subject: 02, n=09 | train_f1: 0.91249 | test_f1: 0.47619\n",
      "Subject: 020, n=04 | train_f1: 0.91099 | test_f1: 0.55556\n",
      "Subject: 021, n=02 | train_f1: 0.94200 | test_f1: 1.0\n",
      "Subject: 022, n=05 | train_f1: 0.89923 | test_f1: 1.0\n",
      "Subject: 023, n=01 | train_f1: 0.89959 | test_f1: 1.0\n",
      "Subject: 024, n=01 | train_f1: 0.95714 | test_f1: 1.0\n",
      "Subject: 026, n=09 | train_f1: 0.87111 | test_f1: 0.47059\n",
      "Subject: 028, n=03 | train_f1: 0.91627 | test_f1: 1.0\n",
      "Subject: 03, n=05 | train_f1: 0.92674 | test_f1: 1.0\n",
      "Subject: 030, n=03 | train_f1: 0.94970 | test_f1: 1.0\n",
      "Subject: 031, n=01 | train_f1: 0.89474 | test_f1: 1.0\n",
      "Subject: 032, n=04 | train_f1: 0.77954 | test_f1: 1.0\n",
      "Subject: 033, n=05 | train_f1: 0.95434 | test_f1: 0.33333\n",
      "Subject: 034, n=03 | train_f1: 0.93398 | test_f1: 1.0\n",
      "Subject: 035, n=08 | train_f1: 0.86992 | test_f1: 0.61111\n",
      "Subject: 036, n=01 | train_f1: 0.87730 | test_f1: 1.0\n",
      "Subject: 037, n=01 | train_f1: 0.88147 | test_f1: 0.0\n",
      "Subject: 04, n=02 | train_f1: 0.94154 | test_f1: 1.0\n",
      "Subject: 05, n=06 | train_f1: 0.89924 | test_f1: 1.0\n",
      "Subject: 06, n=04 | train_f1: 0.90704 | test_f1: 0.33333\n",
      "Subject: 07, n=05 | train_f1: 0.93299 | test_f1: 1.0\n",
      "Subject: 08, n=01 | train_f1: 0.90198 | test_f1: 0.0\n",
      "Subject: 09, n=10 | train_f1: 0.88874 | test_f1: 0.56296\n",
      "Subject: 11, n=04 | train_f1: 0.98181 | test_f1: 1.0\n",
      "Subject: 12, n=11 | train_f1: 0.88911 | test_f1: 0.8963\n",
      "Subject: 13, n=02 | train_f1: 0.92792 | test_f1: 0.33333\n",
      "Subject: 14, n=03 | train_f1: 0.83938 | test_f1: 0.4\n",
      "Subject: 15, n=03 | train_f1: 0.97748 | test_f1: 1.0\n",
      "Subject: 16, n=03 | train_f1: 0.98266 | test_f1: 0.66667\n",
      "Subject: 17, n=31 | train_f1: 0.72525 | test_f1: 0.96727\n",
      "Subject: 19, n=11 | train_f1: 0.87093 | test_f1: 0.91534\n",
      "Subject: 20, n=02 | train_f1: 0.95955 | test_f1: 1.0\n",
      "Subject: 21, n=01 | train_f1: 0.94708 | test_f1: 0.0\n",
      "Subject: 22, n=02 | train_f1: 0.88243 | test_f1: 1.0\n",
      "Subject: 23, n=08 | train_f1: 0.87819 | test_f1: 0.46667\n",
      "Subject: 24, n=03 | train_f1: 0.84135 | test_f1: 0.55556\n",
      "Subject: 25, n=05 | train_f1: 0.91389 | test_f1: 0.55556\n",
      "Subject: 26, n=11 | train_f1: 0.97217 | test_f1: 0.80702\n",
      "Subject: s1, n=06 | train_f1: 0.93709 | test_f1: 0.16667\n",
      "Subject: s11, n=07 | train_f1: 0.94938 | test_f1: 1.0\n",
      "Subject: s12, n=09 | train_f1: 0.88699 | test_f1: 0.61905\n",
      "Subject: s13, n=10 | train_f1: 0.97079 | test_f1: 1.0\n",
      "Subject: s14, n=10 | train_f1: 0.92769 | test_f1: 1.0\n",
      "Subject: s15, n=04 | train_f1: 0.89264 | test_f1: 0.55556\n",
      "Subject: s18, n=07 | train_f1: 0.82054 | test_f1: 1.0\n",
      "Subject: s19, n=02 | train_f1: 0.93427 | test_f1: 1.0\n",
      "Subject: s2, n=06 | train_f1: 0.93014 | test_f1: 0.77778\n",
      "Subject: s20, n=22 | train_f1: 0.94706 | test_f1: 0.64762\n",
      "Subject: s3, n=39 | train_f1: 1.00000 | test_f1: 0.55776\n",
      "Subject: s4, n=19 | train_f1: 0.85714 | test_f1: 0.53162\n",
      "Subject: s5, n=02 | train_f1: 0.89619 | test_f1: 0.33333\n",
      "Subject: s6, n=04 | train_f1: 0.93027 | test_f1: 0.55556\n",
      "Subject: s8, n=13 | train_f1: 0.92792 | test_f1: 0.34524\n",
      "Subject: s9, n=04 | train_f1: 0.92252 | test_f1: 0.73333\n",
      "Total f1: 0.7561748930123292, SMIC: 0.703101275432736, CASME2: 0.8409090909090908, SAMM: 0.6967846967846967\n"
     ]
    }
   ],
   "source": [
    "LOSO(uv_frames, df, epochs=200, lr=0.01, weight_decay=0.001,\n",
    "     dropout=0.5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSO(features, df, epochs=200, lr=0.01, batch_size=128, dropout=0.5, weight_decay=0.001,\n",
    "         verbose=True):\n",
    "    random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed(1)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    outputs_list = []\n",
    "    #groupby reorders elements, now the labels are in same order as outputs\n",
    "    df_groupby = pd.concat([i[1] for i in df.groupby(\"subject\")])\n",
    "    dataset_groupby = df_groupby[\"dataset\"]\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(df[\"emotion\"])\n",
    "    labels_groupby = le.transform(df_groupby[\"emotion\"])\n",
    "\n",
    "    #loop over each subject\n",
    "    for group in df.groupby(\"subject\"):\n",
    "        subject = group[0]\n",
    "        #split data to train and test based on the subject index\n",
    "        train_index = df[df[\"subject\"] != subject].index\n",
    "        X_train = features[train_index, :]\n",
    "        y_train = labels[train_index]\n",
    "        dataset_train = dataset[train_index]\n",
    "        \n",
    "        test_index = df[df[\"subject\"] == subject].index\n",
    "        X_test = features[test_index, :]\n",
    "        y_test = labels[test_index]\n",
    "        dataset_test = dataset[test_index]\n",
    "\n",
    "        #create pytorch dataloaders from the split\n",
    "        megc_dataset_train = MEData(X_train, y_train, dataset_train, None)\n",
    "        dataset_loader_train = torch.utils.data.DataLoader(megc_dataset_train,\n",
    "                                                             batch_size=batch_size, shuffle=True,\n",
    "                                                             num_workers=0)\n",
    "\n",
    "        megc_dataset_test = MEData(X_test, y_test, dataset_test, None)\n",
    "        dataset_loader_test = torch.utils.data.DataLoader(megc_dataset_test,\n",
    "                                                         batch_size=100, shuffle=False,\n",
    "                                                         num_workers=0)\n",
    "\n",
    "        \n",
    "        net = Net(df[\"emotion\"].nunique(), dropout).float().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "        net.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch in dataset_loader_train:\n",
    "                data_batch, labels_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = net(data_batch.float())\n",
    "                loss = criterion(outputs, labels_batch.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        #Test model\n",
    "        net.eval()\n",
    "        data_batch_test, labels_batch_test, _ = dataset_loader_test.__iter__().__next__()\n",
    "        data_batch_test, labels_batch_test = data_batch_test.to(device), labels_batch_test.to(device)\n",
    "        outputs = net(data_batch_test.float())\n",
    "        _, prediction = outputs.max(1)\n",
    "        prediction = prediction.cpu().data.numpy()\n",
    "        outputs_list.append(prediction)\n",
    "        \n",
    "        train_outputs = net(data_batch.float())\n",
    "        _, train_prediction = train_outputs.max(1)\n",
    "        train_prediction = train_prediction.cpu().data.numpy()\n",
    "        train_f1 = f1_score(labels_batch.cpu().data.numpy(), train_prediction, average=\"macro\")\n",
    "        test_f1 = f1_score(labels_batch_test.cpu().data.numpy(), prediction, average=\"macro\")\n",
    "        \n",
    "        \n",
    "        #Print statistics\n",
    "        if verbose:\n",
    "            print(\"Subject: {}, n={} | train_f1: {:.5f} | test_f1: {:.5}\".format(\n",
    "                subject, str(labels_batch_test.shape[0]).zfill(2), train_f1, test_f1))\n",
    "            \n",
    "    outputs = np.concatenate(outputs_list)\n",
    "    f1_total = f1_score(labels_groupby, outputs, average=\"macro\")\n",
    "    idx = dataset_groupby == \"smic\"\n",
    "    f1_smic = f1_score(labels_groupby[idx], outputs[idx], average=\"macro\")\n",
    "    idx = dataset_groupby == \"casme2\"\n",
    "    f1_casme2 = f1_score(labels_groupby[idx], outputs[idx], average=\"macro\")\n",
    "    idx = dataset_groupby == \"samm\"\n",
    "    f1_samm = f1_score(labels_groupby[idx], outputs[idx], average=\"macro\")\n",
    "    print(\"Total f1: {}, SMIC: {}, CASME2: {}, SAMM: {}\".format(f1_total, f1_smic, f1_casme2, f1_samm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
